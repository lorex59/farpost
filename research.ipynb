{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задача - cоздать механизм исправления ошибок в словах пользовательского ввода"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Постановка задачи.\n",
    "\n",
    "Если Х - исходное предложение, которое может содержать орфографические ошибки или опечатки, то алгоритм А должен произвести предложение Y, такое что:\n",
    "1. Ошибки и опечатки исправлены.\n",
    "2. Изначально корректные части предложения остались без изменения.\n",
    "3. Смысл не поменялся.\n",
    "4. Стиль не поменялся.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также необходимо определиться с тем, что же такое опечатка/ошибка.\n",
    "\n",
    "1. Вставка символа. (Пошел гулять -> ПошШел гулять)\n",
    "2. Удаление символа. (Пошел гулять -> Пошл гулять)\n",
    "3. Замена символа. (Пошел гулять -> Пошел гулИть)\n",
    "4. Перестановка двух соседних символов. (Пошел гулять -> ПоЕШл гулять)\n",
    "5. Вставка пробела. (Пошел гулять -> По шел гулять)\n",
    "6. Удаление пробела. (Пошел гулять -> Пошелгулять)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ошибку между строками будем определять с помощью расстояния Дамерау-Левенштейна - это мера разницы двух строк символов, определяемая как минимальное количество операций вставки, удаления, замены и транспозиции (перестановки двух соседних символов), необходимых для перевода одной строки в другую."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вначале были собраны следующие датасеты:\n",
    "\n",
    "1. RUSpellRU - данные собранные из \"Живого Журнала\"\n",
    "2. MultidomainGold - примеры из 7 текстовых источников, включая открытую сеть, новости, социальные сети, обзоры, субтитры, политические документы и литературные произведения\n",
    "3. MedSpellChecker - тексты с ошибками из медицинского анамнеза.\n",
    "4. GitHubTypoCorpusRu - орфографические ошибки и опечатки в коммитах с GitHub;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Fields:\n",
    "* sources (str): оригинальное предложение.\n",
    "* corrections (str): исправленное предложение.\n",
    "* domain (str): домен, из которого взято предложение."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве решения будут использоваться seq2seq модели - это модель, принимающая на вход последовательность элементов (слов, букв, признаков изображения и т.д.) и возвращающая другую последовательность элементов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Алгоритм\n",
    "1. В декодере RNN получает эмбеддинг <END> токена и первоначальное скрытое состояние.\n",
    "2. RNN обрабатывает входной элемент, генерирует выход и новый вектор скрытого состояния (h4). Выход отбрасывается.\n",
    "3. Механизм внимания использует скрытые состояния энкодера и вектор h4 для вычисления контекстного вектора (C4) на данном временном отрезке.\n",
    "4. Вектора h4 и C4 конкатенируются в один вектор.\n",
    "5. Этот вектор пропускается через нейронную сеть прямого распространения (feedforward neural network, FFN), обучаемую совместно с моделью.\n",
    "6. Вывод FFN сети указывает на выходное слово на данном временном отрезке.\n",
    "7. Алгоритм повторяется для следующего временного отрезка."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тестирование модели.\n",
    "\n",
    "В качестве моделей для тестирования былы взяты две модели: RuM2M100-1.2B и RuM2M100-418M\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Я пришел в ДВФУ однако он был закрыт\n"
     ]
    }
   ],
   "source": [
    "from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\n",
    "\n",
    "path_to_model = \"ai-forever/RuM2M100-1.2B\" # Путь к модели на HuggingFace\n",
    "\n",
    "model = M2M100ForConditionalGeneration.from_pretrained(path_to_model)\n",
    "tokenizer = M2M100Tokenizer.from_pretrained(path_to_model, src_lang=\"ru\", tgt_lang=\"ru\")\n",
    "\n",
    "sentence = \"Я пришол в ДВФУ однако он был закрыт\"\n",
    "\n",
    "encodings = tokenizer(sentence, return_tensors=\"pt\")\n",
    "generated_tokens = model.generate(\n",
    "        **encodings, forced_bos_token_id=tokenizer.get_lang_id(\"ru\"))\n",
    "answer = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "print(answer[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Я пришел в ДВФУ, однако он был закрыт.\n"
     ]
    }
   ],
   "source": [
    "from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\n",
    "\n",
    "sentence = \"Я пришол в ДВФУ однако он был закрыт\"\n",
    "\n",
    "path_to_model = \"ai-forever/RuM2M100-418M\"\n",
    "\n",
    "model = M2M100ForConditionalGeneration.from_pretrained(path_to_model)\n",
    "tokenizer = M2M100Tokenizer.from_pretrained(path_to_model, src_lang=\"ru\", tgt_lang=\"ru\")\n",
    "\n",
    "\n",
    "encodings = tokenizer(sentence, return_tensors=\"pt\")\n",
    "generated_tokens = model.generate(\n",
    "        **encodings, forced_bos_token_id=tokenizer.get_lang_id(\"ru\"))\n",
    "answer = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "print(answer[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "M2M100ForConditionalGeneration(\n",
       "  (model): M2M100Model(\n",
       "    (shared): Embedding(14341, 1024, padding_idx=1)\n",
       "    (encoder): M2M100Encoder(\n",
       "      (embed_tokens): Embedding(14341, 1024, padding_idx=1)\n",
       "      (embed_positions): M2M100SinusoidalPositionalEmbedding()\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x M2M100EncoderLayer(\n",
       "          (self_attn): M2M100Attention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): ReLU()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): M2M100Decoder(\n",
       "      (embed_tokens): Embedding(14341, 1024, padding_idx=1)\n",
       "      (embed_positions): M2M100SinusoidalPositionalEmbedding()\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x M2M100DecoderLayer(\n",
       "          (self_attn): M2M100Attention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): M2M100Attention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=14341, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"data/MultidomainGold.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(df, model, encodings, print_bool=False): # Функция для тестирования функционала модели\n",
    "    test_sentences_source = df['source']\n",
    "    test_sentences_correction = df['correction']\n",
    "    answer_from_model_list = []\n",
    "    for sentence in test_sentences_source:\n",
    "        encodings = tokenizer(sentence, return_tensors=\"pt\")\n",
    "        generated_tokens = model.generate(\n",
    "                **encodings, forced_bos_token_id=tokenizer.get_lang_id(\"ru\"))\n",
    "        answer = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "        answer_from_model_list.append(answer)\n",
    "    if print_bool:\n",
    "        for sentence, answer, correction in zip(test_sentences_source, answer_from_model_list, test_sentences_correction):\n",
    "            print(f\"Исходное предложение:     {sentence}\")\n",
    "            print(f\"Исправленное предложение: {answer[0]}\")\n",
    "            print(f\"Корректное предложение:   {correction}\")\n",
    "            print()\n",
    "    \n",
    "    return correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Тестирование RuM2M100-418M\n",
    "path_to_model = \"ai-forever/RuM2M100-418M\"\n",
    "\n",
    "model = M2M100ForConditionalGeneration.from_pretrained(path_to_model)\n",
    "tokenizer = M2M100Tokenizer.from_pretrained(path_to_model, src_lang=\"ru\", tgt_lang=\"ru\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Исходное предложение:     Сувенир с \"инклюзом\" Грибочки из Балтийского янтаря!\n",
      "Исправленное предложение: Сувенир с \"инклюзом\" Грибочки из Балтийского янтаря.\n",
      "Корректное предложение:   Сувенир с \"инклюзом\" Грибочки из Балтийского янтаря!\n",
      "\n",
      "Исходное предложение:     Сама процедура получения ВНЖ начинаеться с регистрации ООО в Словении.\n",
      "Исправленное предложение: Сама процедура получения ВНЖ начинается с регистрации ООО в Словении.\n",
      "Корректное предложение:   Сама процедура получения ВНЖ начинаеться с регистрации ООО в Словении.\n",
      "\n",
      "Исходное предложение:     В минувшие выхожные состоялось торжественное открытие 10-го, юбилейного сезона ГУМ-Катка.\n",
      "Исправленное предложение: В минувшие выходные состоялось торжественное открытие 10-го юбилейного сезона ГУМ-карта.\n",
      "Корректное предложение:   В минувшие выходные состоялось торжественное открытие 10-го, юбилейного сезона ГУМ-Катка.\n",
      "\n",
      "Исходное предложение:     Прооессионал может всегда отличить \"богему\" от \"участника\", т.к. цены на их услуги сильно кусаются, а результат их деятельности однозначен, т.е. в случае работы \"богемы\" интрига пропадает сразу.\n",
      "Исправленное предложение: Профессионал может всегда отличить \"Богему\" от \"участника\". Так цены на их услуги сильно кусаются, а результат их деятельности однозначен... В случае работы \"Богемы\" интрига пропадает сразу...\n",
      "Корректное предложение:   Профессионал может всегда отличить \"богему\" от \"участника\", т. к. цены на их услуги сильно кусаются, а результат их деятельности однозначен, т. е. в случае работы \"богемы\" интрига пропадает сразу.\n",
      "\n",
      "Исходное предложение:     Московская кофейня на паяхъ Espresso кофе растворимый, 95 г Бывает так, что очень хочется настоящего свежемолотого кофе, но приготовить его негде, а под рукой только чайник.\n",
      "Исправленное предложение: Московская кофейня на пляжах Espresso Кофе растворимый, 95 г. Бывает так, что очень хочется настоящего свежемолотого кофе. Но приготовить его негде, а под рукой только чайник.\n",
      "Корректное предложение:   Московская кофейня на паяхъ Espresso кофе растворимый, 95 г Бывает так, что очень хочется настоящего свежемолотого кофе, но приготовить его негде, а под рукой только чайник.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "correction = test_model(test_df[10:15], model, tokenizer, True),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Тестирование RuM2M100-1.2B\n",
    "path_to_model = \"ai-forever/RuM2M100-1.2B\"\n",
    "\n",
    "model = M2M100ForConditionalGeneration.from_pretrained(path_to_model)\n",
    "tokenizer = M2M100Tokenizer.from_pretrained(path_to_model, src_lang=\"ru\", tgt_lang=\"ru\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Исходное предложение:     Сувенир с \"инклюзом\" Грибочки из Балтийского янтаря!\n",
      "Исправленное предложение: Сувенир с \"инклюзом\" Грибочки из Балтийского янтаря!\n",
      "Корректное предложение:   Сувенир с \"инклюзом\" Грибочки из Балтийского янтаря!\n",
      "\n",
      "Исходное предложение:     Сама процедура получения ВНЖ начинаеться с регистрации ООО в Словении.\n",
      "Исправленное предложение: Сама процедура получения ВНЖ начинаеться с регистрации ООО в Словении.\n",
      "Корректное предложение:   Сама процедура получения ВНЖ начинаеться с регистрации ООО в Словении.\n",
      "\n",
      "Исходное предложение:     В минувшие выхожные состоялось торжественное открытие 10-го, юбилейного сезона ГУМ-Катка.\n",
      "Исправленное предложение: В минувшие выходные состоялось торжественное открытие 10-го, юбилейного сезона ГУМ-Катка.\n",
      "Корректное предложение:   В минувшие выходные состоялось торжественное открытие 10-го, юбилейного сезона ГУМ-Катка.\n",
      "\n",
      "Исходное предложение:     Прооессионал может всегда отличить \"богему\" от \"участника\", т.к. цены на их услуги сильно кусаются, а результат их деятельности однозначен, т.е. в случае работы \"богемы\" интрига пропадает сразу.\n",
      "Исправленное предложение: Профессионал может всегда отличить \"богему\" от \"участника\", т.к. цены на их услуги сильно кусаются, а результат их деятельности однозначен, т.е. в случае работы \"богемы\" интрига пропадает сразу.\n",
      "Корректное предложение:   Профессионал может всегда отличить \"богему\" от \"участника\", т. к. цены на их услуги сильно кусаются, а результат их деятельности однозначен, т. е. в случае работы \"богемы\" интрига пропадает сразу.\n",
      "\n",
      "Исходное предложение:     Московская кофейня на паяхъ Espresso кофе растворимый, 95 г Бывает так, что очень хочется настоящего свежемолотого кофе, но приготовить его негде, а под рукой только чайник.\n",
      "Исправленное предложение: Московская кофейня на паях Espresso кофе растворимый, 95 г Бывает так, что очень хочется настоящего свежемолотого кофе, но приготовить его негде, а под рукой только чайник.\n",
      "Корректное предложение:   Московская кофейня на паяхъ Espresso кофе растворимый, 95 г Бывает так, что очень хочется настоящего свежемолотого кофе, но приготовить его негде, а под рукой только чайник.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "correction = test_model(test_df[10:15], model, tokenizer, True),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вывод: Можем заметить, что обе модели работают неплохо и ошибки возникают только со специфичными и редкими словами.\n",
    "\n",
    "RuM2M100-1.2B работает в два раза медленне RuM2M100-418M, поэтому я бы использовал вторую модель.\n",
    "\n",
    "RuM2M100-1.2B из-за большего количество параметров способен разпознавать сложные опечатки, но я считаю, что подобные ошибки будут крайне редко, поэтому модели RuM2M100-418M должно быть достаточно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В main.py буду использовать RuM2M100-418M"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
